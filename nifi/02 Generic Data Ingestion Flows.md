# Generic Data Ingestion Flows

Let us understand how to build Generic Data Ingestion flows in this session.

- Pre-requisites
- Recap of simple pipeline
- Recap of Flowfiles and Attributes
- Overview of Scheduling
- Files to Flowfiles - Options
- Copying Flowfiles to Files
- Deleting Files
- Build Generic Pipeline
- Quick Overview of NiFi Expression Language

## Pre-requisites

Here are the pre-requisites before we get into the session. We will be using single node development server to explore NiFi in-depth.

- We have an environment where Hadoop and NiFi are setup.
- Run `jps` to ensure whether HDFS Components and NiFi are running or not.
- In case if HDFS and NiFi processes are not running, run following commands to get the environment ready.

```bash
start-dfs.sh
start-yarn.sh
start-nifi.sh
# You can run jps and see if all the Java processes related to HDFS and NiFi are running or not.
```

- We also need to have **retail_db** data setup under **/home/savas/data** folder. Run `ls -ltr /home/savas/data/retail_db` to verify that we have the data we need. You can also run `find /home/savas/data/retail_db -type f` to list the files in `retail_db` folder.

- For the OS user we are using we need to have user space under HDFS. You can validate by running `hdfs dfs -ls /user/$USER`

You may need to delete folders from HDFS if they exist (from previous runs):

```bash
  hdfs dfs -rm -R -skipTrash /user/savas/get
  hdfs dfs -rm -R -skipTrash /user/savas/retail_db
```

You also need to copy the data to the `/tmp` folder for some of the examples:

```bash
cp -r /home/savas/data/retail_db /tmp
```

## Recap of simple pipeline

Earlier we have understood simple data ingestion pipeline using NiFi.

- Processor Group
- Processors
- Settings
- Scheduling
- Properties
- Connecting multiple Processors

## Recap of Flowfiles and Attributes

Let us recap of core data structures as part of NiFi.

- Flowfiles are the ones which will contain the data.
- Attributes consists of metadata generated by processors.
- We can review the contents of flowfiles using Data Provenance of the processor.
- We typically don't have to manipulate flowfiles related to our data as part of the ingestion process. However NiFi provide processors to manipulate the data in flowfiles as well.
- In this session we will ingest the data with out applying any transformations.

## Overview of Scheduling

NiFi supports 2 types of scheduling.

- Timer based
- Cron based

## Files to Flowfiles - Options

Here are the design patterns with respect to gettig data from files to flowfiles.

- List + Fetch (ListFile + FetchFile)
- Get (GetFile)
- Processors which start with **List** (for example ListFile) does not take any input and hence it can be first processor in our ingestion pipeline to copy files.
- **List** processors typically read metadata of the files and NiFi will maintain the **state** to get the delta files (files that are added since last trigger).
- We need to use corresponding **Fetch** processor to fetch the files using the path and file name from upstream **List** Processors.
- We can directly use **Get** processor to get the files. Typically they also does not take input.
- However, unlike **List** processors, **Get** processors does not maintain state. We need to take care of capturing the delta.
  We can get data from local file system, HDFS, AWS S3, Azure Blob, SFTP and many more. All of them provide both the options (List + Fetch or Get).

Flowfiles can also be saved to Files in all supported file systems.

## Copying Flowfiles to Files

We can copy files to target file systems using **Put** Processors.

- **PutHDFS** can be used to save files to HDFS.
- **PutFile** can be used to save files to local file system.
- We can also save files to cloud based file systems such as **S3**, **Azure Blob** etc.
- We will also have options such as compression to compress files as they are placed into target file system.
- For processors like **PutHDFS** we also have replacement strategy (fail, replace, ignore, append etc).

## Deleting Files

We can also delete files from any standard file system using **Delete** Processors.

- **DeleteHDFS** can be used to delete files from HDFS.
- Similar processors are available to delete files from other file systems.
- We need to have write permissions to delete files from underlying file systems.

## Build Generic Pipeline

We will be building multiple pipelines to understand different design patterns.

#### ListFile + FetchFile -> HDFS

Here is how we can build pipeline to get the files from local file system to HDFS.

- Use **ListFile** to get the files recursively.
- Configure `Age Attributes` as per requirement. The `Minimum File Age` attribute is used to avoid reading partial files by requiring the file creation timestamp to be at least N units of time. For example if we set this to 60 seconds, NiFi will read the file when it is at least 1 minute old. The `Maximum File Age` is used to avoid reading the same files after a restart (or clearing of the processor state). In this case we are saying: Don't read old files. We can set that to something like 1 or 2 days.
- Use **FetchFile** to fetch files.
- Use **UpdateAttribute** to define target location using Update Attribute Processor. We can use NiFi Expression Language for the same. We can define a `targetDataDir` attribute with the following expression as its value:

```
${absolute.path:substringBeforeLast('/'):substringAfterLast('/')}
```

- Use **PutHDFS** to save the files in the HDFS. We can simulate source directory structure while writing data to target file system.
- We need to pass HDFS configuration files to **PutHDFS**. In some cases we might have to provide **classpaths** as well.

#### ListFile + FetchFile -> HDFS with Archival Strategy

Here is how we can build pipeline to get the files from local file system to HDFS with archival strategy. All the files that are fetched from local folder **/home/savas/data/retail_db** should be moved to **/archive/retail_db**.

- Use **ListFile** to get the files recursively.
- Configure Age Attributes as per requirement.
- Use **FetchFile** to fetch files. We can configure achival strategy using **Completion Strategy**
  - Set **Completion Strategy** to **Move File**.
  - Set the path to which the files should be moved to. For this you can use expression `${absolute.path:replaceFirst('/home/savas/data/', '/archive/')}` for **Move Destination Directory**.
- Update attribute to define target location using Update Attribute Processor. We can use NiFi Expression Language for the same.

```
${absolute.path:substringBeforeLast('/'):substringAfterLast('/')}
```

- Use **PutHDFS** to save the files in the HDFS. We can simulate source directory structure while writing data to target file system.
- We need to pass HDFS configuration files to **PutHDFS**. In some cases we might have to provide **classpaths** as well.

#### GetFile -> HDFS with Archival Strategy.

Here is how we can build pipeline to get the files from local file system to HDFS with archival strategy. All the files that are fetched from local folder **/home/savas/data/retail_db** should be moved to **/archive/retail_db**.

- Use **GetFile** to get the files recursively.
- Configure Age Attributes as per requirement.
- Set **Keep Source File** to **false**. If not **GetFile** read all the files every time.
- Create a branch **PutFile** to specify the path to which the files should be copied to for archival process.
- Use **PutHDFS** to save the files in the HDFS. We can simulate source directory structure while writing data to target file system by using NiFi expression language.

```
/user/savas/get/retail_db/${absolute.path:substringBeforeLast('/'):substringAfterLast('/')}
```

- We need to pass HDFS configuration files to **PutHDFS**. In some cases we might have to provide **classpaths** as well.

#### ListHDFS + FetchHDFS -> Local File System

In some cases, we might have to copy the data from HDFS to local file system.

- We can also use **GetHDFS** in place of **ListHDFS + FetchHDFS**, however we need to ensure that we have archival strategy as **GetHDFS** does not maintain the state and will read all the files in the source directory.
- We can use **PutFile** to copy files to the target location - **/tmp/data/fromhdfs/retail_db**. Here is the code using NiFi expression language to update the path dynamically.

```
${path:replace('/user/savas/', '/tmp/data/fromhdfs/')}
```

## Quick Overview of NiFi Expression Language

Let us understand more about NiFi Expression Language.

- It is extensively used to mutate flowfiles or attributes.
- We can also use NiFi Expression Language as part of conditional flows using processors such as **Route On Attribute**.
- NiFi Expression Language provides robust set of functions for data manipulation as well as traversing to JSON Paths.
- We have used expression language as part of previous topics in this module. We will see more as we get into more complex modules.
